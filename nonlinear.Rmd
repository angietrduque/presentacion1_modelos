---
title: "*Asociaciones no lineales en el modelo Normal*"
author:  "Angie Rodríguez Duque & César Saavedra Vanegas"
date: "Octubre 30 de 2020"
output:
  ioslides_presentation:
    widescreen: true 
    smaller: true 
    transition: slower
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r warning=FALSE, include=F, paged.print=TRUE}
suppressMessages(library(deSolve))
suppressMessages(library(nlme))
suppressMessages(library(ggplot2))
suppressMessages(library(plotly))
suppressMessages(library(reshape2))
suppressMessages(library(plotly))
suppressMessages(library(psych))
suppressMessages(library(GGally))

```


# Introducción

##
<div style="text-align: justify">

Hasta ahora solo hemos considerado asociaciones lineales entre X e Y, donde un aumento de delta en una variable explicativa continua $x_{i}$ produce el mismo cambio $\beta_{1}$ en y para todos los valores de $x_{i}$. $\beta_{1}$ a veces se denomina **"Slope"** porque es un gradiente lineal. Una ecuación de regresión lineal simple con una sola pendiente lineal es:

**Regresión lineal simple:** Consiste en generar un modelo de regresión (ecuación de una recta) que permita explicar la relación lineal que existe entre dos variables. 

$$Y=\beta_{0}+\beta_{1}X_{1}+\epsilon$$

* $\beta_{0}:$ La ordenada en el origen.

* $\beta_{1}:$ La pendiente.

* $\epsilon:$ El error aleatorio.

**Regresión lineal múltiple:** Es una extensión de la regresión lineal simple. Permite generar un modelo lineal en el que el valor de la variable dependiente o respuesta $(Y)$ se determina a partir de un conjunto de variables independientes llamadas predictores $(X_{1}, X_{2}, X_{3},...)$. 

$$Y_{i}=(\beta_{0}+\beta_{1}X_{1i}+\beta_{2}X_{2i}+...+\beta_{n}X_{ni})+\epsilon_{i}$$

<div>

##
<div style="text-align: justify">

### **Centramiento de las variables**

En la práctica, cuando se utilizan transformaciones como la cuadrática, que pueden crear valores grandes de $x_{i}$, puede resultar útil centrar las variables explicativas utilizando su media (x) y escalarlas utilizando su desviación estándar (de). Para mayor comodidad de notación, primero creamos una versión centrada y escalada de $x_i$:

$$\tilde{x}_{i}=\displaystyle{\frac{(x_{i}-\bar{x}_{i})}{sd}}$$


### **Ventajas**

* Una ventaja adicional del centrado es que la estimación de la intersección $\beta_{0}$ ahora relaciona el valor de y promedio con el valor de $x$ promedio en lugar del valor de $y$ promedio cuando $x$ es cero, lo que puede no ser significativo si $x$ no puede ser cero. **Ejemplo:** El peso de una persona.

* Además, los parámetros de **"Slope"** ahora representan un cambio de una desviación estándar que es potencialmente más significativo que un cambio de una sola unidad que puede ser muy pequeño o grande.

* Por último, escalar por la desviación estándar facilita la comparación de la importancia de las variables.

<div/>

# Regresión polinómica

##
<div style="text-align: justify">

Antes de aplicar un modelo de regresión lineal simple, se hace necesario conocer si los datos se pueden ajustar a un modelo de regresión lineal, es decir conocer el grado de asociación entre la variable de respuesta y las variables predictoras y a su vez poder determinar la proporción de variabilidad existente entre la variable dependiente explicada por la variable independiente. 

Una asociación en forma de U se puede modelar agregando una versión cuadrática de la variable y un parámetro $\beta$ adicional:

**Regresión polinómica:** Incorporan flexibilidad a un modelo lineal introduciendo nuevos predictores obtenidos al elevar a distintas potencias el predictor original.

$$y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x^{2}_{i}+\beta_{3}x^{3}_{i}+...+\beta_{d}x^{d}_{i}+\epsilon_{i}$$

Y se ajusta al modelo:

$$E(Y_{i})=\beta_{0}+\beta_{1}\tilde{x}_{i}+\beta_{2}\tilde{x}^{2}_{i}$$

<div>

##
<div style="text-align: justify">

### **Correlación lineal**

Para estudiar la relación lineal existente entre dos variables continuas es necesario disponer de parámetros que permitan cuantificar dicha relación. Uno de estos parámetros es la covarianza, que indica el grado de variación conjunta de dos variables aleatorias.

$$Covarianza \hspace{0.2cm} muestral=Cov(X,Y)= \frac{\displaystyle\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$$
siendo $\bar{x}$ e $\bar{y}$ la media de cada variable y $x_i$ e $y_i$ el valor de las variables para la observación i.

### **Características:**

+ Todos ellos varían entre +1 y -1. Siendo +1 una correlación positiva perfecta y -1 una correlación negativa perfecta.

+ Se emplean como medida de fuerza de asociación (tamaño del efecto):
    -0: asociación nula.
    -0.1: asociación pequeña.
    -0.3: asociación mediana.
    -0.5: asociación moderada.
    -0.7: asociación alta.
    -0.9: asociación muy alta.

<div>

##
<div style="text-align: justify">

Las principales diferencias entre estos tres coeficientes de asociación son:

+ La correlación de Pearson funciona bien con variables cuantitativas que tienen una distribución normal. En el libro Handbook of Biological Statatistics se menciona que sigue siendo bastante robusto a pesar de la falta de normalidad. Es más sensible a los valores extremos que las otras dos alternativas.

+ La correlación de Spearman se emplea cuando los datos son ordinales, de intervalo, o bien cuando no se satisface la condición de normalidad para variables continuas y los datos se pueden transformar a rangos. Es un método no paramétrico.

+ La correlación de Kendall es otra alternativa no paramétrica para el estudio de la correlación que trabaja con rangos. Se emplea cuando se dispone de pocos datos y muchos de ellos ocupan la misma posición en el rango, es decir, cuando hay muchas ligaduras.

<div>

##
<div style="text-align: justify">

### **Coeficiente de correlación de Pearson**

Se utiliza para estudiar la asociación entre un factor de estudio y una variable de respuesta cuantitativa, mide el grado de asociación entre dos variables tomando valores entre $-1$ y $1$.

+ **Población:**

$$\rho=\frac{Cov(X,Y)}{\sigma_{x}\sigma_{y}} $$
+ **Muestra:**

$$r_{xy}=\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}}} $$

<div/>

##
<div style="text-align: justify">

### **Coeficiente de correlación de Spearman**

$$r_{s}=1-\frac{6\sum d^{2}_{i}}{n(n^{2}-1)}$$

Siendo $d_{i}$ la distancia entre los rangos de cada observación $(x_{i}−y_{i})$ y $n$ el número de observaciones.

<div/>

<div style="text-align: justify">

### **Coeficiente Tau de Kendall**

$$\tau=\frac{C-D}{\displaystyle\frac{1}{2}n(n-1)}$$

Siendo $C$ el número de pares concordantes, aquellos en los que el rango de la segunda variable es mayor que el rango de la primera variable. $D$ el número de pares discordantes, cuando el rango de la segunda es igual o menor que el rango de la primera variable.

<div/>

# Ejemplo en R
##
<div style="text-align: justify">

### **Regresión polinomial**

Una marca de coches quiere generar un modelo de regresión que permita predecir el consumo de combustible *(mpg)* en función de la potencia del motor *(horsepower)*.

<center>

```{r warning=FALSE, include=T, paged.print=TRUE}
library(ISLR)
attach(Auto)

# Variable dicotomica calidadAB
hp <- vector()
hp[Auto$horsepower <= 150] <- "bajo"
hp[Auto$horsepower > 150] <- "alto"
hp <- as.factor(hp)
```


```{r warning=FALSE, include=F, paged.print=F}
G1 <- ggplot(Auto, aes(x = horsepower, y = mpg)) + geom_point(colour = "black")
```

```{r fig.height=3, fig.width=8, warning=FALSE, include=T, paged.print=FALSE}
G1
```

<div style="text-align: justify">
La representación gráfica de los datos muestra una fuerte asociación entre el consumo y la potencia del motor. La distribución de las observaciones apunta a que la relación entre ambas variables tiene cierta curvatura, por lo que un modelo lineal no puede captarla por completo.

<div>

## **Modelo lineal**

<div style="text-align: justify">
```{r message=F, warning=F, include=T, paged.print=FALSE}
attach(Auto)
modelo_1 <- glm(formula = mpg ~ hp, data = Auto, family = "binomial")
```
<div>

```{r warning=F, include=T, paged.print=F}
summary(modelo_1)
```


##

<div style="text-align: justify">

<center>
```{r warning=FALSE, include=F, paged.print=F}
G2 <- ggplot(Auto, aes(x = horsepower, y = mpg)) + geom_point(colour = "black") + 
  geom_smooth(method='lm', se = FALSE)

```

```{r fig.height=4, fig.width=8, message=F, warning=FALSE, include=T, paged.print=FALSE}
ggplotly(G2)
```


<div style="text-align: justify">

Una forma de incorporar asociaciones no lineales a un modelo lineal es mediante transformaciones de los predictores incluidos en el modelo, por ejemplo, elevándolos a distintas potencias. En este caso, el tipo de curvatura es de tipo cuadrática, por lo que un polinomio de segundo grado podría mejorar el modelo.

<div>

##

<div style="text-align: justify">

En R se pueden generar modelos de regresión polinómica de diferentes formas:

+ Identificando cada elemento del polinomio: 

$$modelo\_pol2 \leftarrow lm(formula = mpg \sim horsepower + I(horsepower^{2}), data = Auto)$$ 

**Nota:** El uso de I() es necesario ya que el símbolo $^$ tiene otra función dentro de las fórmulas de R.

+ Con la función poly(): 

$$lm(formula = mpg \sim poly(horsepower, 2), data = Auto)$$

## **Modelo Polinómico**

```{r warning=FALSE, include=T, paged.print=F}
modelo_cuadratico <- lm(formula = mpg ~ poly(horsepower, 2), data = Auto)
summary(modelo_cuadratico)
```

<div>

##
<div style="text-align: justify">

<center>
```{r warning=FALSE, include=T, paged.print=F}
par(mfrow = c(2, 2))
plot(modelo_cuadratico)
```

<div style="text-align: justify">

El valor $R^{2}$ del modelo cuadrático $(0.6876)$ es mayor que el obtenido con el modelo lineal simple $(0.6059)$ y el p-value del término cuadrático es altamente significativo. Se puede concluir que el modelo cuadrático recoge mejor la verdadera relación entre el consumo de los vehículos y la potencia de su motor.

<div>

##
<div style="text-align: justify">

Al tratarse de modelos anidados, es posible emplear un ANOVA para contrastar la hipótesis nula de que ambos modelos se ajustan a los datos igual de bien.


```{r warning=FALSE, include=T, paged.print=F}
anova(modelo_lineal, modelo_cuadratico)
```

El ANOVA muestra claras evidencias de que incluir el término cuadrático mejora el modelo.

<div>

##
<div style="text-align: justify">

<center>
```{r warning=FALSE, include=F, paged.print=F}
library(ggplot2)
G3 <- ggplot(Auto, aes(x = horsepower, y = mpg)) +
    geom_point(colour = "black") +
    stat_smooth(method = "lm", formula = y ~ poly(x, 2), colour = "red", se=F) 
```

```{r warning=FALSE, include=T, paged.print=F}
ggplotly(G3)
```


<div>

## 

<center>
```{r warning=FALSE, include=F, paged.print=F}
G4 <- ggplot(Auto, aes(x = horsepower, y = mpg)) + geom_point(colour = "black") +
  stat_smooth(method = 'lm', formula = y ~ poly(x, 1), aes(colour = 'Lineal'), se = FALSE) +
  stat_smooth(method = 'lm', formula = y ~ poly(x, 2), aes(colour = 'polynomial grado 2'), se= FALSE) +
  stat_smooth(method = 'lm', formula = y ~ poly(x, 5), aes(colour = 'polynomial grado 5'), se = FALSE) +
  stat_smooth(method = 'lm', formula = y ~ poly(x, 10), aes(colour = 'polynomial grado 10'), se = FALSE)
  
```

```{r fig.height=6, fig.width=9, warning=FALSE, include=T, paged.print=F}
ggplotly(G4)
```
<div>

##
<div style="text-align: justify">

### **Conclusiones:**

+ La elección del grado del polinomio influye directamente en la flexibilidad del modelo. 

    - Cuanto mayor es el grado del polinomio más se ajusta el modelo a las observaciones, un polinomio de grado $n^{\circ} observaciones-1$ pasa por todos los puntos. 
    - Por lo tanto, es importante no excederse en el grado del polinomio para no causar problemas de overfitting (no suele recomendarse ir más allá de grado 3-4). 

+ Existen varias estrategias para identificar el grado óptimo del polinomio:

    - Incrementar secuencialmente el orden del polinomio hasta que la nueva incorporación no sea significativa.

    - Iniciar el proceso con un polinomio de grado alto e ir eliminando secuencialmente, de mayor a menor los términos no significativos.

    - Emplear un ANOVA para comparar cada nuevo modelo con el de orden inferior y determinar si la mejora es significativa.

    - Emplear cross-validation.

<div>


# Bibliografía
##

* Dobson, A. J., & Barnett, A. G. (2018). An introduction to generalized linear models. CRC press.


# ¡Gracias por tu atención!
